{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972437f7",
   "metadata": {},
   "source": [
    "Install chromadb and tiktoken packages using pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7156ff",
   "metadata": {},
   "source": [
    "PART 1: Web Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254d546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./openai_key\")\n",
    "openai_key = f.read().strip()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cf3ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' UbiComp 2020'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "# Document loader\n",
    "loader = WebBaseLoader(\"https://fsalim.github.io\")\n",
    "# Index that wraps above steps\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "# Question-answering\n",
    "question = \"At which conference was Flora the PC Co-chair? \"\n",
    "index.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4967a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Flora has received 11 awards.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How many awards has Flora received, as per her homepage? \"\n",
    "index.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b074a",
   "metadata": {},
   "source": [
    "PART 2: Running chains on a LLM on your machine: LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1898263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install speechrecognition and pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea325eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "494bddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/aditya/Documents/GitHub/llama/llama-7b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 128\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5287.72 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =   64.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 64  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path =\"/Users/aditya/Documents/GitHub/llama/llama-7b.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=128,\n",
    "    f16_kv=True, \n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5393097a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c6fdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_llama = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "487e534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, Mark has a few advantages over Elon in that he built a successful company (Facebook) while Elon has yet to build a commercially viable product on his own; however, Elon is working on it now with the Tesla and SpaceX projects, so both are still \"works in progress.\"  But for Mark's sake, let's say he has 15-20 years head start on this, so that"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14223.83 ms\n",
      "llama_print_timings:      sample time =   122.95 ms /    94 runs   (    1.31 ms per token,   764.52 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15294.35 ms /    31 tokens (  493.37 ms per token,     2.03 tokens per second)\n",
      "llama_print_timings:        eval time = 685601.68 ms /    93 runs   ( 7372.06 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time = 701988.23 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' First, Mark has a few advantages over Elon in that he built a successful company (Facebook) while Elon has yet to build a commercially viable product on his own; however, Elon is working on it now with the Tesla and SpaceX projects, so both are still \"works in progress.\"  But for Mark\\'s sake, let\\'s say he has 15-20 years head start on this, so that'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter = \"Who is the better CEO: Mark Zuckerberg or Elon Musk? \"\n",
    "llm_chain_llama.run(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08cc165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer in exactly two words: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2402db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/aditya/Documents/GitHub/llama/llama-7b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 33\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 5287.72 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =   16.50 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_batch = 32\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path =\"/Users/aditya/Documents/GitHub/llama/llama-7b.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=33,\n",
    "    f16_kv=True, \n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm_chain_llama = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3868758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Elon"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15680.60 ms\n",
      "llama_print_timings:      sample time =     3.53 ms /     3 runs   (    1.18 ms per token,   850.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15680.39 ms /    30 tokens (  522.68 ms per token,     1.91 tokens per second)\n",
      "llama_print_timings:        eval time = 16207.50 ms /     2 runs   ( 8103.75 ms per token,     0.12 tokens per second)\n",
      "llama_print_timings:       total time = 31913.30 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' [Elon'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter = \"Who is the better CEO: Mark Zuckerberg or Elon Musk? \"\n",
    "llm_chain_llama.run(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb823a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
